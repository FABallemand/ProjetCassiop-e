{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(images, captions=None, cmap=None):\n",
    "    \"\"\"\n",
    "    Plot n images using subplots\n",
    "    \"\"\"\n",
    "    f, axes = plt.subplots(1, len(images), sharey=True)\n",
    "    f.set_figwidth(15)\n",
    "    for ax,image in zip(axes, images):\n",
    "        ax.imshow(image, cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed\n",
    "SEED = 30\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Dataset path\n",
    "DATASET_PATH = \"../data/hand_gesture_recog/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(os.path.join(DATASET_PATH, \"five/hand1(21).jpg\"))\n",
    "print(type(img))\n",
    "print(img.shape)\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gesture_path = os.path.join(DATASET_PATH, \"five\")\n",
    "print(gesture_path)\n",
    "\n",
    "gestures = glob.glob(os.path.join(gesture_path, \"*\"))\n",
    "print(gestures)\n",
    "\n",
    "rand_index = random.randint(0, len(gestures))\n",
    "img = cv2.imread(gestures[rand_index])\n",
    "img = cv2.resize(img, (100, 100))\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gestures_paths = glob.glob(os.path.join(DATASET_PATH, \"*\"))\n",
    "gestures_paths.sort()\n",
    "print(gestures_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SAMPLE_PER_GESTURE = 1600\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "label_to_class = {}\n",
    "\n",
    "for i, gestures_path in enumerate(gestures_paths):\n",
    "    gesture_type = gestures_path.split(\"/\")[-1]\n",
    "    gestures = glob.glob(os.path.join(gesture_path, \"*\"))\n",
    "    label = [0] * len(gestures_paths)\n",
    "    label[i] = 1\n",
    "    label_to_class[i] = gesture_type\n",
    "    for j, gesture in enumerate(gestures):\n",
    "        if j == MAX_SAMPLE_PER_GESTURE:\n",
    "            break\n",
    "        img = cv2.imread(gesture)\n",
    "        img = cv2.resize(img,(100, 100))            # Resize\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # Gray scale\n",
    "        images.append(img)\n",
    "        labels.append(label)\n",
    "    \n",
    "print(len(images))\n",
    "print(len(labels))\n",
    "print(len(label_to_class))\n",
    "print(label_to_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization\n",
    "from keras.layers import Activation, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Model\n",
    "model = Sequential()\n",
    "\n",
    "# First conv layer\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation=\"relu\", input_shape=(100, 100, 1))) \n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Second conv layer\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Softmax layer\n",
    "model.add(Dense(len(label_to_class), activation=\"softmax\"))\n",
    "\n",
    "# Model summary\n",
    "optimiser = Adam()\n",
    "model.compile(optimizer=optimiser, loss=\"categorical_crossentropy\", metrics=[\"categorical_accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray(images)\n",
    "y = np.asarray(labels)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=4)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 100, 120, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 100, 120, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,\n",
    "          y_train,\n",
    "          batch_size=64,\n",
    "          epochs=64,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test))\n",
    "\n",
    "model.save(\"hand_gesture_recog_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model(\"hand_gesture_recog_model.keras\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gesture_path = os.path.join(DATASET_PATH, \"thumbsdown\")\n",
    "print(gesture_path)\n",
    "\n",
    "gestures = glob.glob(os.path.join(gesture_path, \"*\"))\n",
    "print(gestures)\n",
    "\n",
    "rand_index = random.randint(0, len(gestures))\n",
    "img = cv2.imread(gestures[rand_index])\n",
    "img = cv2.resize(img,(100, 120))\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img.shape)\n",
    "img =  img.reshape(-1, 100, 120, 1)\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.argmax(model.predict(img), axis=1)\n",
    "print(f\"{pred[0]}: {label_to_class[pred[0]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
